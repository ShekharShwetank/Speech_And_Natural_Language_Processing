{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e194f95",
   "metadata": {},
   "source": [
    "# LAB 4\n",
    "\n",
    "    SHWETANK SHEKHAR\n",
    "    22BEC1204\n",
    "\n",
    "## TASK:\n",
    "    Implement sentiment classifier using both BoW and N-gram language \n",
    "    models, observe the difference in accuracies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd1633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e23ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"C:\\Users\\shwet\\Downloads\\archive\\movie.csv\")\n",
    "sentences = dataset['text'].tolist()\n",
    "sentiment = dataset['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ce8c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text, stop_words):\n",
    "    clean = re.sub(r'<[^>]+>', '', text)\n",
    "    clean = re.sub(r'[^a-zA-Z\\s]', '', clean.lower())\n",
    "    words = word_tokenize(clean)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_sentences = [preprocess(s, stop_words) for s in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7071dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary and index mapping for BoW\n",
    "def create_vocab(sentences):\n",
    "    vocab = set()\n",
    "    for s in sentences:\n",
    "        words = s.split()\n",
    "        vocab.update(words)\n",
    "    vocab = sorted(list(vocab))\n",
    "    return {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "word_to_index = create_vocab(processed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627a43d",
   "metadata": {},
   "source": [
    "BOW-VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ad31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed BoW vectorization using sparse matrix\n",
    "def create_bow_vectors_sparse(sentences, word_to_index):\n",
    "    num_sentences = len(sentences)\n",
    "    vocab_size = len(word_to_index)\n",
    "    vectors = lil_matrix((num_sentences, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for i, s in enumerate(sentences):\n",
    "        words = s.split()\n",
    "        for word in words:\n",
    "            if word in word_to_index:\n",
    "                idx = word_to_index[word]\n",
    "                vectors[i, idx] += 1\n",
    "    \n",
    "    return vectors.tocsr()\n",
    "\n",
    "X_bow = create_bow_vectors_sparse(processed_sentences, word_to_index)\n",
    "y = np.array(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e64705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Model Results\n",
      "Accuracy: 0.87875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      3966\n",
      "           1       0.88      0.88      0.88      4034\n",
      "\n",
      "    accuracy                           0.88      8000\n",
      "   macro avg       0.88      0.88      0.88      8000\n",
      "weighted avg       0.88      0.88      0.88      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred_bow = clf_bow.predict(X_test_bow)\n",
    "print(\"BoW Model Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(classification_report(y_test, y_pred_bow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a36719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_bow(text, model, word_to_index, stop_words):\n",
    "    vector = lil_matrix((1, len(word_to_index)), dtype=np.int32)\n",
    "    clean = re.sub(r'<[^>]+>', '', text)\n",
    "    clean = re.sub(r'[^a-zA-Z\\s]', '', clean.lower())\n",
    "    words = word_tokenize(clean)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    for word in words:\n",
    "        if word in word_to_index:\n",
    "            vector[0, word_to_index[word]] += 1\n",
    "    prediction = model.predict(vector)\n",
    "    return \"Positive\" if prediction[0] == 1 else \"Negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa5e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for ' This movie was absolutely wonderful! ': Positive\n",
      "Prediction for ' I hated this movie, it was so boring. ': Negative\n"
     ]
    }
   ],
   "source": [
    "test_sentence_pos = \"This movie was absolutely wonderful!\"\n",
    "test_sentence_neg = \"I hated this movie, it was so boring.\"\n",
    "print(\"Prediction for '\", test_sentence_pos, \"':\", predict_bow(test_sentence_pos, clf_bow, word_to_index, stop_words))\n",
    "print(\"Prediction for '\", test_sentence_neg, \"':\", predict_bow(test_sentence_neg, clf_bow, word_to_index, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459c8b1",
   "metadata": {},
   "source": [
    "N-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd9af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "def create_ngram_vectors_sparse(sentences, ngram_to_index):\n",
    "    num_sentences = len(sentences)\n",
    "    vocab_size = len(ngram_to_index)\n",
    "    vectors = lil_matrix((num_sentences, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for i, s in enumerate(sentences):\n",
    "        tokens = s.split()\n",
    "        ngrams = generate_ngrams(tokens, 1) + generate_ngrams(tokens, 2)\n",
    "        for ng in ngrams:\n",
    "            if ng in ngram_to_index:\n",
    "                vectors[i, ngram_to_index[ng]] += 1\n",
    "    \n",
    "    return vectors.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51698ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create N-gram vocabulary\n",
    "all_ngrams = []\n",
    "for sentence in processed_sentences:\n",
    "    tokens = sentence.split()\n",
    "    unigrams = generate_ngrams(tokens, 1)\n",
    "    bigrams = generate_ngrams(tokens, 2)\n",
    "    all_ngrams.extend(unigrams + bigrams)\n",
    "    \n",
    "ngram_counter = Counter(all_ngrams)\n",
    "top_k = 20000\n",
    "vocab = [ng for ng, _ in ngram_counter.most_common(top_k)]\n",
    "ngram_to_index = {ng: i for i, ng in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b001f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate N-gram vectors\n",
    "X_ngram = create_ngram_vectors_sparse(processed_sentences, ngram_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82bd60c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " N-gram Model Results\n",
      "Accuracy: 0.879125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      3966\n",
      "           1       0.88      0.88      0.88      4034\n",
      "\n",
      "    accuracy                           0.88      8000\n",
      "   macro avg       0.88      0.88      0.88      8000\n",
      "weighted avg       0.88      0.88      0.88      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_ngram, X_test_ngram, y_train, y_test = train_test_split(X_ngram, y, test_size=0.2, random_state=42)\n",
    "clf_ngram = LogisticRegression(max_iter=1000)\n",
    "clf_ngram.fit(X_train_ngram, y_train)\n",
    "\n",
    "y_pred_ngram = clf_ngram.predict(X_test_ngram)\n",
    "print(\"\\n N-gram Model Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
    "print(classification_report(y_test, y_pred_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "505e2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ngram(text, model, ngram_to_index, stop_words):\n",
    "    clean = re.sub(r'<[^>]+>', '', text)\n",
    "    clean = re.sub(r'[^a-zA-Z\\s]', '', clean.lower())\n",
    "    tokens = word_tokenize(clean)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    vector = lil_matrix((1, len(ngram_to_index)), dtype=np.int32)\n",
    "    ngrams = generate_ngrams(tokens, 1) + generate_ngrams(tokens, 2)\n",
    "    for ng in ngrams:\n",
    "        if ng in ngram_to_index:\n",
    "            vector[0, ngram_to_index[ng]] += 1\n",
    "    \n",
    "    prediction = model.predict(vector)\n",
    "    return \"Positive\" if prediction[0] == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a54f326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for ' This movie was absolutely wonderful! ': Positive\n",
      "Prediction for ' I hated this movie, it was so boring. ': Negative\n"
     ]
    }
   ],
   "source": [
    "test_sentence_pos = \"This movie was absolutely wonderful!\"\n",
    "test_sentence_neg = \"I hated this movie, it was so boring.\"\n",
    "print(\"Prediction for '\", test_sentence_pos, \"':\", predict_ngram(test_sentence_pos, clf_ngram, ngram_to_index, stop_words))\n",
    "print(\"Prediction for '\", test_sentence_neg, \"':\", predict_ngram(test_sentence_neg, clf_ngram, ngram_to_index, stop_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
